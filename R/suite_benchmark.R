#' A benchmark suite with which to run all the methods on the different tasks
#'
#' @param task_ids The ids of the tasks to be used in the evaluation.
#' @param methods A tibble of TI methods to use, see \code{\link[dynmethods]{get_ti_methods}}.
#' @param parameters A named list containing data frames of the parameters to evaluate.
#'   The names of the list must be equal to the \code{short_name} column of \code{methods}.
#'   The data frames must correspond to the parameter set \code{par_set} of each method,
#'   e.g. as generated by \code{\link[ParamHelpers]{generateDesignOfDefaults}},
#'   but must also contain the column \code{paramset_id}.
#' @param timeout_per_execution The maximum number of seconds each execution is allowed to run, otherwise it will fail.
#' @param max_memory_per_execution The maximum amount of memory each execution is allowed to use, otherwise it will fail.
#' @param metrics Which metrics to evaluate; see \code{\link{calculate_metrics}} for a list of which metrics are available.
#' @param num_repeats The number of times to repeat the evaluation.
#' @param local_output_folder A folder in which to output intermediate and final results.
#' @param remote_output_folder A folder in which to store intermediate results in a remote directory when using the PRISM package.
#' @param execute_before Shell commands to execute before running R.
#' @param verbose Whether or not to print extra information.
#'
#' @importFrom PRISM qsub_lapply override_qsub_config
#' @importFrom pbapply pblapply
#' @importFrom readr read_rds write_rds
#'
#' @export
benchmark_submit <- function(
  task_ids,
  methods,
  parameters,
  timeout_per_execution,
  max_memory_per_execution,
  metrics,
  num_repeats,
  local_output_folder,
  remote_output_folder,
  execute_before = NULL,
  verbose = FALSE
) {
  benchmark_submit_check(
    local_tasks_folder,
    remote_tasks_folder,
    task_ids,
    methods,
    parameters,
    timeout_per_execution,
    max_memory_per_execution,
    metrics,
    num_repeats,
    local_output_folder,
    remote_output_folder,
    execute_before,
    verbose
  )

  ## prepare for remote execution; create a qsub config
  qsub_config <- PRISM::override_qsub_config(
    wait = FALSE,
    remove_tmp_folder = FALSE,
    stop_on_error = FALSE,
    verbose = FALSE,
    num_cores = 1,
    memory = max_memory_per_execution,
    max_wall_time = timeout_per_execution,
    r_module = NULL,
    execute_before = execute_before,
    local_tmp_path = local_output_folder,
    remote_tmp_path = remote_output_folder
  )

  ## run evaluation for each method separately
  map(seq_len(nrow(methods)), function(methodi) {
    method <- dynutils::extract_row_to_list(methods, methodi)

    # determine where to store certain outputs
    method_folder <- paste0(local_output_folder, "/", method$short_name)
    output_file <- paste0(method_folder, "/output.rds")
    qsubhandle_file <- paste0(method_folder, "/qsubhandle.rds")

    PRISM:::mkdir_remote(path = method_folder, remote = "")

    ## If no output or qsub handle exists yet
    if (!file.exists(output_file) && !file.exists(qsubhandle_file)) {
      cat("Submitting ", method$name, "\n", sep="")

      # fetch parameters
      parms <- parameters[[method$short_name]]

      ## create a grid for each of the tasks, paramsets, and repeats
      grid <- crossing(
        task_id = task_ids,
        paramset_id = parms$paramset_id,
        repeat_i = seq_len(num_repeats)
      )

      # set parameters for the cluster
      qsub_config_method <-
        PRISM::override_qsub_config(
          qsub_config = qsub_config,
          name = paste0("D_", method$short_name),
          local_tmp_path = paste0(method_folder, "/r2gridengine")
        )

      # which packages to load on the cluster
      qsub_packages <- c("dplyr", "purrr", "dyneval", "readr")

      # which data objects will need to be transferred to the cluster
      qsub_environment <-  c(
        "grid", "remote_tasks_folder", "remote_output_folder", "parms", "method", "metrics", "verbose"
      )

      # submit to the cluster
      qsub_handle <- PRISM::qsub_lapply(
        X = seq_len(nrow(grid)),
        object_envir = environment(),
        qsub_environment = qsub_environment,
        qsub_packages = qsub_packages,
        qsub_config = qsub_config_method,
        FUN = benchmark_qsub_fun
      )

      # save data and handle to RDS file
      metadata <- lst(
        local_output_folder,
        remote_output_folder,
        task_ids,
        method,
        timeout_per_execution,
        max_memory_per_execution,
        metrics,
        num_repeats,
        grid,
        method_folder,
        output_file,
        qsubhandle_file,
        parms,
        qsub_handle
      )
      readr::write_rds(metadata, qsubhandle_file)

      NULL
    }
  })

  invisible()
}

#' @importFrom testthat expect_equal expect_is
#' @importFrom ParamHelpers dfRowToList
benchmark_submit_check <- function(
  local_tasks_folder,
  remote_tasks_folder,
  task_ids,
  methods,
  parameters,
  timeout_per_execution,
  max_memory_per_execution,
  metrics,
  num_repeats,
  local_output_folder,
  remote_output_folder,
  execute_before,
  verbose
) {
  # check tasks
  # TODO: check whether all tasks are present, local and remote

  # check methods
  testthat::expect_is(methods, "tbl")

  # check parameters
  testthat::expect_is(parameters, "list")
  testthat::expect_equal(sort(names(parameters)), sort(methods$short_name))

  # check parameters
  for (mi in seq_len(nrow(methods))) {
    # fetch par_set and short_name
    par_set <- methods$par_set[[mi]]
    short_name <- methods$short_name[[mi]]

    cat("Checking parameters for ", sQuote(short_name), "\n", sep = "")

    # extract given_parameters, and check that paramset_id is present
    given_params <- parameters[[short_name]]
    testthat::expect_true("paramset_id" %in% colnames(given_params))

    # convert given params to list to avoid problems with vector parameters
    param_list <-
      given_params %>%
      select(-paramset_id) %>%
      ParamHelpers::dfRowToList(par_set, 1)

    # filter tunable params in par_set
    tunable_pars <- par_set$pars

    # compare that indeed the params in param_list and tunable_pars are the same
    testthat::expect_true(all(names(param_list) %in% names(tunable_pars)))
  }

  # check timeout_per_execution
  testthat::expect_is(timeout_per_execution, "numeric")

  # check max_memory_per_execution
  testthat::expect_is(max_memory_per_execution, "character")
  testthat::expect_match(max_memory_per_execution, "[0-9]+G")
}

#' Helper function for benchmark suite
#'
#' @param grid_i Benchmark config index
benchmark_qsub_fun <- function(grid_i) {
  # call helper function
  benchmark_run_evaluation(grid, grid_i, remote_tasks_folder, parms, method, metrics, verbose)
}

#' @importFrom readr read_rds
#' @importFrom ParamHelpers dfRowToList trafoValue
benchmark_run_evaluation <- function(
  grid,
  grid_i,
  remote_tasks_folder,
  parms,
  method,
  metrics,
  verbose
) {
  # fetch grid arguments
  tid <- grid[grid_i,]$task_id
  pid <- grid[grid_i,]$paramset_id

  # read task
  task <- readr::read_rds(paste0(remote_tasks_folder, "/", tid, ".rds"))

  # get param set
  parm_df <- parms %>% filter(paramset_id == pid)
  parm_list <- ParamHelpers::dfRowToList(
    df = parm_df %>% select(-paramset_id),
    par.set = method$par_set,
    i = 1
  ) %>%
    ParamHelpers::trafoValue(method$par_set, .)

  # start evaluation
  out <- evaluate_ti_method(
    tasks = task,
    method = method,
    parameters = parm_list,
    metrics = metrics,
    extra_metrics = NULL,
    output_model = TRUE,
    mc_cores = 1,
    verbose = verbose
  ) %>% attr("extras")

  # create summary
  bind_cols(
    grid[grid_i,] %>% select(-task_id),
    tibble(
      grid_i,
      params_notrafo = list(parm_df),
      params_trafo = list(parm_list),
      model = out$.models
    ),
    out$.summary %>%
      mutate(error_message = ifelse(is.null(error[[1]]), "", error[[1]]$message)) %>%
      select(-error)
  )
}

#' Fetch the results of the benchmark jobs from the cluster.
#'
#' @param local_output_folder The folder in which to output intermediate and final results.
#'
#' @importFrom PRISM qsub_retrieve qacct qstat_j
#' @importFrom readr read_rds write_rds
#' @export
benchmark_fetch_results <- function(local_output_folder) {
  method_names <- list.dirs(local_output_folder, full.names = FALSE, recursive = FALSE) %>% discard(~ . == "")

  # process each method separately
  map(method_names, function(method_name) {
    method_folder <- paste0(local_output_folder, "/", method_name)
    output_metrics_file <- paste0(method_folder, "/output_metrics.rds")
    output_models_file <- paste0(method_folder, "/output_models.rds")
    qsubhandle_file <- paste0(method_folder, "/qsubhandle.rds")

    # if the output has not been processed yet, but a qsub handle exists,
    # attempt to fetch the results from the cluster
    if (!file.exists(output_metrics_file) && file.exists(qsubhandle_file)) {

      cat(method_name, ": Attempting to retrieve output from cluster: ", sep = "")
      metadata <- readr::read_rds(qsubhandle_file)
      grid <- metadata$grid
      qsub_handle <- metadata$qsub_handle
      num_tasks <- qsub_handle$num_tasks

      # attempt to retrieve results; return NULL if job is still busy or has failed
      output <- PRISM::qsub_retrieve(
        qsub_handle,
        wait = FALSE
      )

      if (!is.null(output)) {
        cat("Output found! Saving output.\n", sep = "")

        suppressWarnings({
          qacct_out <- PRISM::qacct(qsub_handle)
        })

        # process each job separately
        outputs <- map_df(seq_len(nrow(grid)), function(grid_i) {
          out <- output[[grid_i]]

          if (length(out) != 1 || !is.na(out)) {
            # hooray, the benchmark suite ran fine!
            out

          } else {
            # if qacct is empty or the correct taskid cannot be found, then
            # this job never ran
            if (is.null(qacct_out) || !any(qacct_out$taskid == grid_i)) {
              qsub_error <- "Job cancelled by user"
            } else {
              qacct_filt <- qacct_out %>% filter(taskid == grid_i) %>% arrange(desc(row_number_i)) %>% slice(1)

              qacct_memory <- qacct_filt$maxvmem %>% str_replace("GB$", "") %>% as.numeric
              qacct_exit_status <- qacct_filt$exit_status %>% str_replace(" +", " ")
              qacct_exit_status_number <- qacct_exit_status %>% str_replace(" .*", "")
              qacct_user_time <- qacct_filt$ru_stime %>% str_replace("s$", "") %>% as.numeric

              qsub_memory <- qsub_handle$memory %>% str_replace("G$", "") %>% as.numeric
              qsub_user_time <- qsub_handle$max_wall_time

              memory_messages <- c("cannot allocate vector of size", "MemoryError")
              is_memory_problem <- function(message) {
                any(map_lgl(memory_messages, ~grepl(., message)))
              }

              qsub_error <-
                if (qacct_exit_status_number %in% c("134", "139") || is_memory_problem(attr(out, "qsub_error"))) {
                  "Memory limit exceeded"
                } else if (qacct_exit_status_number %in% c("137", "140", "9", "64")) {
                  "Time limit exceeded"
                } else if (qacct_exit_status_number != "0") {
                  qacct_exit_status
                } else {
                  attr(out, "qsub_error")
                }
            }

            # create a method that will just return the error generated by qsub
            method_failer <- metadata$method
            method_failer$run_fun <- function(...) {
              stop(qsub_error)
            }
            formals(method_failer$run_fun) <- formals(metadata$method$run_fun)

            # "rerun" the evaluation, in order to generate the expected output except with
            # the default fail-scores for each of the metrics
            out <- benchmark_run_evaluation(
              grid = grid,
              grid_i = grid_i,
              remote_tasks_folder = metadata$local_tasks_folder,
              parms = metadata$parms,
              method = method_failer,
              metrics = metadata$metrics,
              verbose = FALSE
            )
          }
        })

        # save models separately
        models <- outputs$model
        model_ids <- map_chr(models, function(model) {
          if (!is.null(model)) {
            model$id
          } else {
            NA
          }
        })
        models <- models %>% setNames(model_ids)
        outputs <- outputs %>% select(-model) %>% mutate(model_i = seq_len(n()), model_id = model_ids)
        readr::write_rds(models, output_models_file)

        # save output
        readr::write_rds(outputs, output_metrics_file)

      } else {
        # the job is probably still running
        suppressWarnings({
          qstat_out <- PRISM::qstat_j(qsub_handle)
        })

        error_message <-
          if (is.null(qstat_out) || nrow(qstat_out) > 0) {
            "job is still running"
          } else {
            "qsub_retrieve of results failed -- no output was produced, but job is not running any more"
          }

        cat("Output not found. ", error_message, ".\n", sep = "")
      }

      NULL
    } else {
      if (file.exists(output_metrics_file)) {
        cat(method_name, ": Output already present.\n", sep = "")
      } else {
        cat(method_name, ": No qsub file was found.\n", sep = "")
      }
      NULL
    }

  })

  # return nothing
  invisible()
}


#' Gather and bind the results of the benchmark jobs
#'
#' @param local_output_folder The folder in which to output intermediate and final results.
#' @param load_models Whether or not to load the models as well.
#'
#' @importFrom readr read_rds
#' @export
benchmark_bind_results <- function(local_output_folder, load_models = FALSE) {
  method_names <- list.dirs(local_output_folder, full.names = FALSE, recursive = FALSE) %>% discard(~ . == "")

  # process each method separately
  as_tibble(map_df(method_names, function(method_name) {
    method_folder <- paste0(local_output_folder, method_name)
    output_metrics_file <- paste0(method_folder, "/output_metrics.rds")
    output_models_file <- paste0(method_folder, "/output_models.rds")

    if (file.exists(output_metrics_file)) {
      cat(method_name, ": Reading previous output\n", sep = "")
      output <- readr::read_rds(output_metrics_file)

      # read models, if requested
      if (load_models && file.exists(output_models_file)) {
        models <- readr::read_rds(output_models_file)
        output$model <- models
      }

      output
    } else {
      cat(method_name, ": Output not found, skipping\n", sep = "")
      NULL
    }
  }))
}
